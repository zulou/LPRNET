{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4ca7a-a3ac-4870-b8d9-88aee14d0298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 8\n",
      "Train Len is 1373\n",
      "batch size is ---- 8\n",
      "Training ...\n",
      "Start of epoch 0 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737558647.172018   47625 cuda_dnn.cc:529] Loaded cuDNN version 90600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 232.25452148082644, time 43.66825866699219 \n",
      "\n",
      "Start of epoch 1 / 100\n",
      "Train loss 186.00751193734104, time 41.39356708526611 \n",
      "\n",
      "Start of epoch 2 / 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import utils\n",
    "import math\n",
    "from model import LPRNet\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '/kaggle/input/your-dataset-name')\n",
    "\n",
    "os.environ['CUDA_DIR'] = '/usr/lib/cuda'\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/lib/cuda'\n",
    "\n",
    "train_epochs_var = 100\n",
    "batch_size_var  = 8\n",
    "val_batch_size_var = 4\n",
    "train_dir_var = \"./train\"\n",
    "val_dir_var = \"./valid\"\n",
    "#pretrained = \n",
    "lr_var = 1e-3\n",
    "decay_steps_var = 500\n",
    "decay_rate_var = 0.995\n",
    "staircase_var = \"smooth\"\n",
    "saved_dir_var = \"./saved_models\"\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initiate the Neural Network\n",
    "    net = LPRNet(NUM_CLASS)\n",
    "\n",
    "    # Get the train and validation batch size from argument parser\n",
    "    batch_size = batch_size_var\n",
    "    print(\"batch size is {}\".format(batch_size_var))\n",
    "    val_batch_size = val_batch_size_var\n",
    "\n",
    "    # Initialize the custom data generator\n",
    "    train_gen = utils.DataIterator(img_dir=train_dir_var, batch_size=batch_size)\n",
    "    val_gen = utils.DataIterator(img_dir=val_dir_var, batch_size=val_batch_size_var)\n",
    "\n",
    "    # Variable initialization used for custom training loop\n",
    "    train_len = len(next(os.walk(train_dir_var))[2])\n",
    "    val_len = len(next(os.walk(val_dir_var))[2])\n",
    "    print(\"Train Len is\", train_len)\n",
    "\n",
    "    # Calculate batches per epoch\n",
    "    BATCH_PER_EPOCH = int(math.ceil(train_len / batch_size_var))\n",
    "    print(\"batch size is ---- {}\".format(batch_size_var))\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    tensorboard = keras.callbacks.TensorBoard(\n",
    "        log_dir='tmp/my_tf_logs',\n",
    "        histogram_freq=0,\n",
    "        write_graph=True\n",
    "    )\n",
    "\n",
    "    val_batch_len = int(math.floor(val_len / val_batch_size_var))\n",
    "    evaluator = evaluate.Evaluator(val_gen, net, CHARS, val_batch_len, val_batch_size_var)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # If a pretrained model is available, load weights from it\n",
    "    #if pretrained:\n",
    "    #    net.load_weights(pretrained)\n",
    "\n",
    "    model = net.model\n",
    "    tensorboard.set_model(model)\n",
    "\n",
    "    # Initialize the learning rate\n",
    "    learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
    "        lr,\n",
    "        decay_steps=decay_steps_var,\n",
    "        decay_rate=decay_rate_var,\n",
    "        staircase=staircase_var\n",
    "    )\n",
    "\n",
    "    # Define training optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_var)\n",
    "    print('Training ...')\n",
    "    train_loss = 0\n",
    "\n",
    "    # Starting the training loop\n",
    "    for epoch in range(train_epochs_var):\n",
    "        print(\"Start of epoch {} / {}\".format(epoch, train_epochs_var))\n",
    "\n",
    "        # Zero out the train_loss and val_loss at the beginning of every loop\n",
    "        train_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in range(BATCH_PER_EPOCH):\n",
    "            # Get a batch of images/labels\n",
    "            train_inputs, train_targets, train_labels = train_gen.next_batch()\n",
    "            train_inputs = train_inputs.astype('float32')\n",
    "            train_targets = tf.SparseTensor(train_targets[0], train_targets[1], train_targets[2])\n",
    "\n",
    "            # Open a GradientTape to record the operations run during the forward pass\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get model outputs\n",
    "                logits = model(train_inputs, training=True)\n",
    "\n",
    "                # Pass the model outputs into the CTC loss function\n",
    "                logits = tf.reduce_mean(logits, axis=1)\n",
    "                logits_shape = tf.shape(logits)\n",
    "                cur_batch_size = logits_shape[0]\n",
    "                timesteps = logits_shape[1]\n",
    "                seq_len = tf.fill([cur_batch_size], timesteps)\n",
    "                logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "                # Calculate CTC loss\n",
    "                ctc_loss = tf.nn.ctc_loss(\n",
    "                    labels=train_targets,\n",
    "                    logits=logits,\n",
    "                    logit_length=seq_len,\n",
    "                    label_length=tf.fill([cur_batch_size], tf.shape(train_targets.values)[0]),\n",
    "                    blank_index=NUM_CLASS - 1  # Índice en blanco\n",
    "                )\n",
    "                loss_value = tf.reduce_mean(ctc_loss)\n",
    "\n",
    "            # Calculate gradients and update them\n",
    "            grads = tape.gradient(ctc_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            train_loss += float(loss_value)\n",
    "\n",
    "        tim = time.time() - start_time\n",
    "        print(\"Train loss {}, time {} \\n\".format(float(train_loss / BATCH_PER_EPOCH), tim))\n",
    "\n",
    "        # Run a validation loop every 25 epochs\n",
    "        if epoch != 0 and epoch % 25 == 0:\n",
    "            val_loss = evaluator.evaluate()\n",
    "            # If the validation loss is less than the previous best validation loss, update the saved model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                net.save_weights(os.path.join(saved_dir_var, \"new_out_model_best.weights.h5\"))  # Corregido aquí\n",
    "                print(\"Weights updated in {}/{}\".format(saved_dir_var, \"new_out_model_best.weights.h5\"))\n",
    "            else:\n",
    "                print(\"Validation loss is greater than best_val_loss\")\n",
    "\n",
    "    # Save the final model\n",
    "    net.save(os.path.join(saved_dir_var, \"new_out_model_last.weights.h5\"))  # Corregido aquí\n",
    "    print(\"Final Weights saved in {}/{}\".format(saved_dir_var, \"new_out_model_last.weights.h5\"))\n",
    "    tensorboard.on_train_end(None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parser_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--train_dir\", default=\"./train\", help=\"path to the train directory\")\n",
    "    parser.add_argument(\"--val_dir\", default=\"./valid\", help=\"path to the validation directory\")\n",
    "\n",
    "    parser.add_argument(\"--train_epochs\", type=int, help=\"number of training epochs\", default=151)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"batch size (train)\")\n",
    "    parser.add_argument(\"--val_batch_size\", type=int, default=4, help=\"Validation batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"initial learning rate\")\n",
    "    parser.add_argument(\"--decay_steps\", type=float, default=500, help=\"learning rate decay rate\")\n",
    "    parser.add_argument(\"--decay_rate\", type=float, default=0.995, help=\"learning rate decay rate\")\n",
    "    parser.add_argument(\"--staircase\", action=\"store_true\", help=\"learning rate decay on step (default:smooth)\")\n",
    "\n",
    "    parser.add_argument(\"--pretrained\", help=\"pretrained model location\")\n",
    "    parser.add_argument(\"--saved_dir\", default=\"saved_models\", help=\"folder for saving models\")\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #args = parser_args()\n",
    "    CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    NUM_CLASS = len(CHARS) + 1\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d794d-6060-4603-9c1f-2d70c24f6b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
